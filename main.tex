\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage{neurips_2019}
     
     \RequirePackage[l2tabu,orthodox]{nag}
     
     % SETUP
     \usepackage[utf8]{inputenc}
     \usepackage{algorithmic}
     \usepackage{algorithm}
     
     % {{{ etex }}}
     \usepackage{etex}
     % {{{ common }}}
     \usepackage{xspace,enumerate}
     \usepackage[dvipsnames]{xcolor}
     \usepackage[T1]{fontenc}
     \usepackage[full]{textcomp}
     % {{{ babelamerican }}}
     \usepackage[american]{babel}
     % {{{ mathtools }}}99
     \usepackage{mathtools}
     \usepackage{thmtools}
     \usepackage{thm-restate}
     % {{{ boldmath }}}
     % fix for "too many math alphabets" problem
     \newcommand\hmmax{0} % default 3
     % \usepackage{bm}
     % \usepackage{stmaryrd}
     % {{{ amsthm }}}
     \usepackage{amsthm}
     \newtheorem{theorem}{Theorem}[section]
     \newtheorem*{theorem*}{Theorem}
     \newtheorem{subclaim}{Claim}[theorem]
     \newtheorem{proposition}[theorem]{Proposition}
     \newtheorem*{proposition*}{Proposition}
     \newtheorem{lemma}[theorem]{Lemma}
     \newtheorem*{lemma*}{Lemma}
     \newtheorem{corollary}[theorem]{Corollary}
     \newtheorem*{conjecture*}{Conjecture}
     \newtheorem{fact}[theorem]{Fact}
     \newtheorem*{fact*}{Fact}
     \newtheorem{hypothesis}[theorem]{Hypothesis}
     \newtheorem*{hypothesis*}{Hypothesis}
     \newtheorem{conjecture}[theorem]{Conjecture}
     \theoremstyle{definition}
     \newtheorem{definition}[theorem]{Definition}
     \newtheorem*{definition*}{Definition}
     \newtheorem{algorithms}{Algorithm}
     \newtheorem{construction}[theorem]{Construction}
     \newtheorem{example}[theorem]{Example}
     \newtheorem{question}[theorem]{Question}
     \newtheorem{openquestion}[theorem]{Open Question}
     \newtheorem{problem}[theorem]{Problem}
     \newtheorem{model}[theorem]{Model}
     \newtheorem{protocol}[theorem]{Protocol}
     \newtheorem{assumption}[theorem]{Assumption}
     \theoremstyle{remark}
     \newtheorem{claim}[theorem]{Claim}
     \newtheorem*{claim*}{Claim}
     \newtheorem{remark}[theorem]{Remark}
     \newtheorem*{remark*}{Remark}
     \newtheorem{observation}[theorem]{Observation}
     \newtheorem*{observation*}{Observation}
     % {{{ geometry-nice }}}
%     \usepackage[
%     letterpaper,
%     top=1.2in,
%     bottom=1.2in,
%     left=1in,
%     right=1in]{geometry}
     % {{{ newpx }}}
     \usepackage{newpxtext} % T1, lining figures in math, osf in text
     \usepackage{textcomp} % required for special glyphs
     \usepackage[varg,bigdelims]{newpxmath}
     \usepackage[scr=rsfso]{mathalfa}% \mathscr is fancier than \mathcal
     \usepackage{bm} % load after all math to give access to bold math
     % \useosf %no longer needed
     \linespread{1.1}% Give Palatino more leading (space between lines)
     \let\mathbb\varmathbb
     % {{{ microtype }}}
     \usepackage{microtype}
     \usepackage[
     pagebackref,
     % letterpaper=true,
     colorlinks=true,
     urlcolor=blue,
     linkcolor=blue,
     citecolor=OliveGreen,
     ]{hyperref}
     \usepackage[capitalise,nameinlink]{cleveref}
     \crefname{lemma}{Lemma}{Lemmas}
     \crefname{fact}{Fact}{Facts}
     \crefname{theorem}{Theorem}{Theorems}
     \crefname{corollary}{Corollary}{Corollaries}
     \crefname{claim}{Claim}{Claims}
     \crefname{example}{Example}{Examples}
     \crefname{algorithms}{Algorithm}{Algorithms}
     \crefname{problem}{Problem}{Problems}
     \crefname{definition}{Definition}{Definitions}
     \usepackage{paralist}
     \usepackage{turnstile}
     \usepackage{mdframed}
     \usepackage{tikz}
     \usepackage{caption}
     \DeclareCaptionType{Algorithm}
     \usepackage{newfloat}
     
     % MACROS
     
     %\newcommand{\Authornote}[2]{}
     %\newcommand{\Authornotecolored}[3]{{\leavevmode\color{#1} #2: #3}}
     \newcommand{\Authornotecolored}[3]{}
     \newcommand{\fixme}[1]{\Authornotecolored{red}{[FIX ME}{\leavevmode\color{blue}#1]}}
     %\newcommand{\fixme}[1]{}
     % \newcommand{\Authorcomment}[2]{}
     % \newcommand{\Authorfnote}[2]{}
     % \newcommand{\Dnote}{\Authornote{D}}
     % \newcommand{\Dcomment}{\Authorcomment{D}}
     % \newcommand{\Dfnote}{\Authorfnote{D}}
     % \newcommand{\Pnote}[1]{\Authornotecolored{blue}{[P}{#1]}}
     \newcommand{\SK}[1]{}
     \newcommand{\Pnote}[1]{}
     % \definecolor{forestgreen(traditional)}{rgb}{0.0, 0.27, 0.13}
     % \newcommand{\SK}[1]{{\color{ForestGreen}(S: #1)}}
     % \newcommand{\inote}{\Inote}
     % \newcommand{\Gnote}{\Authornote{G}}
     % \newcommand{\Gfnote}{\Authorfnote{G}}
     % \newcommand{\Pfnote}{\Authorfnote{P}}
     % \newcommand{\PRnote}{\Authornote{PR}}
     % \newcommand{\PRfnote}{\Authorfnote{PR}}
     % \newcommand{\PTnote}{\Authornote{PT}}
     % \newcommand{\PTfnote}{\Authorfnote{PT}}
     % \newcommand{\PGnote}{\Authornote{PG}}
     % \newcommand{\PGfnote}{\Authorfnote{PG}}
     
     % \newcommand{\Bnote}{\Authornote{B}}
     % \newcommand{\Bfnote}{\Authorfnote{B}}
     % \newcommand{\Jnote}{\Authornotecolored{forestgreen(traditional)}{J}}
     % \newcommand{\Jfnote}{\Authorfnote{J}}
     % \newcommand{\Jcomment}{\Authorcomment{J}}
     % \newcommand{\Mnote}{\Authornote{M}}
     % \newcommand{\Mfnote}{\Authorfnote{M}}
     % \newcommand{\Rnote}{\Authornote{M}}
     % \newcommand{\Rfnote}{\Authorfnote{M}}
     % \newcommand{\Nnote}{\Authornote{N}}
     % \newcommand{\Nfnote}{\Authorfnote{N}}
     % \newcommand{\Snote}{\Authornote{S}}
     % \newcommand{\Sfnote}{\Authorfnote{S}}
     \usepackage{boxedminipage}
     % example:
     % \center \noindent\begin{boxedminipage}{1.0\linewidth}}
     % content
     % \end{boxedminipage}
     % \noindent
     % {{{ parentheses }}}
     % various bracket-like commands
     % round parentheses
     \newcommand{\paren}[1]{(#1)}
     \newcommand{\Paren}[1]{\left(#1\right)}
     \newcommand{\bigparen}[1]{\big(#1\big)}
     \newcommand{\Bigparen}[1]{\Big(#1\Big)}
     % square brackets
     \newcommand{\brac}[1]{[#1]}
     \newcommand{\Brac}[1]{\left[#1\right]}
     \newcommand{\bigbrac}[1]{\big[#1\big]}
     \newcommand{\Bigbrac}[1]{\Big[#1\Big]}
     \newcommand{\Biggbrac}[1]{\Bigg[#1\Bigg]}
     % absolute value
     \newcommand{\abs}[1]{\lvert#1\rvert}
     \newcommand{\Abs}[1]{\left\lvert#1\right\rvert}
     \newcommand{\bigabs}[1]{\big\lvert#1\big\rvert}
     \newcommand{\Bigabs}[1]{\Big\lvert#1\Big\rvert}
     % cardinality
     \newcommand{\card}[1]{\lvert#1\rvert}
     \newcommand{\Card}[1]{\left\lvert#1\right\rvert}
     \newcommand{\bigcard}[1]{\big\lvert#1\big\rvert}
     \newcommand{\Bigcard}[1]{\Big\lvert#1\Big\rvert}
     % set
     \newcommand{\set}[1]{\{#1\}}
     \newcommand{\Set}[1]{\left\{#1\right\}}
     \newcommand{\bigset}[1]{\big\{#1\big\}}
     \newcommand{\Bigset}[1]{\Big\{#1\Big\}}
     % norm
     \newcommand{\norm}[1]{\lVert#1\rVert}
     \newcommand{\Norm}[1]{\left\lVert#1\right\rVert}
     \newcommand{\bignorm}[1]{\big\lVert#1\big\rVert}
     \newcommand{\Bignorm}[1]{\Big\lVert#1\Big\rVert}
     \newcommand{\Biggnorm}[1]{\Bigg\lVert#1\Bigg\rVert}
     % 2-norm
     \newcommand{\normt}[1]{\norm{#1}_2}
     \newcommand{\Normt}[1]{\Norm{#1}_2}
     \newcommand{\bignormt}[1]{\bignorm{#1}_2}
     \newcommand{\Bignormt}[1]{\Bignorm{#1}_2}
     % 2-norm squared
     \newcommand{\snormt}[1]{\norm{#1}^2_2}
     \newcommand{\Snormt}[1]{\Norm{#1}^2_2}
     \newcommand{\bigsnormt}[1]{\bignorm{#1}^2_2}
     \newcommand{\Bigsnormt}[1]{\Bignorm{#1}^2_2}
     % norm squared
     \newcommand{\snorm}[1]{\norm{#1}^2}
     \newcommand{\Snorm}[1]{\Norm{#1}^2}
     \newcommand{\bigsnorm}[1]{\bignorm{#1}^2}
     \newcommand{\Bigsnorm}[1]{\Bignorm{#1}^2}
     % 1-norm
     \newcommand{\normo}[1]{\norm{#1}_1}
     \newcommand{\Normo}[1]{\Norm{#1}_1}
     \newcommand{\bignormo}[1]{\bignorm{#1}_1}
     \newcommand{\Bignormo}[1]{\Bignorm{#1}_1}
     % infty-norm
     \newcommand{\normi}[1]{\norm{#1}_\infty}
     \newcommand{\Normi}[1]{\Norm{#1}_\infty}
     \newcommand{\bignormi}[1]{\bignorm{#1}_\infty}
     \newcommand{\Bignormi}[1]{\Bignorm{#1}_\infty}
     % inner product
     \newcommand{\iprod}[1]{\langle#1\rangle}
     \newcommand{\Iprod}[1]{\left\langle#1\right\rangle}
     \newcommand{\bigiprod}[1]{\big\langle#1\big\rangle}
     \newcommand{\Bigiprod}[1]{\Big\langle#1\Big\rangle}
     % {{{ probability }}}
     % expectation, probability, variance
     \newcommand{\Esymb}{\mathbb{E}}
     \newcommand{\Psymb}{\mathbb{P}}
     \newcommand{\Vsymb}{\mathbb{V}}
     \DeclareMathOperator*{\E}{\Esymb}
     \DeclareMathOperator*{\Var}{\Vsymb}
     \DeclareMathOperator*{\ProbOp}{\Psymb}
     \renewcommand{\Pr}{\ProbOp}
     %\newcommand{\given}{\;\middle\vert\;}
     \newcommand{\given}{\mathrel{}\middle\vert\mathrel{}}
     %\newcommand{\given}{\mathrel{}\middle|\mathrel{}}
     % {{{ miscmacros }}}
     % middle delimiter in the definition of a set
     \newcommand{\suchthat}{\;\middle\vert\;}
     % tensor product
     \newcommand{\tensor}{\otimes}
     % add explanations to math displays
     \newcommand{\where}{\text{where}}
     \newcommand{\textparen}[1]{\text{(#1)}}
     \newcommand{\using}[1]{\textparen{using #1}}
     \newcommand{\smallusing}[1]{\text{(\small using #1)}}
     \newcommand{\by}[1]{\textparen{by #1}}
     % spectral order (Loewner order)
     \newcommand{\sge}{\succeq}
     \newcommand{\sle}{\preceq}
     % smallest and largest eigenvalue
     \newcommand{\lmin}{\lambda_{\min}}
     \newcommand{\lmax}{\lambda_{\max}}
     \newcommand{\signs}{\set{1,-1}}
     \newcommand{\varsigns}{\set{\pm 1}}
     \newcommand{\maximize}{\mathop{\textrm{maximize}}}
     \newcommand{\minimize}{\mathop{\textrm{minimize}}}
     \newcommand{\subjectto}{\mathop{\textrm{subject to}}}
     \renewcommand{\ij}{{ij}}
     % symmetric difference
     \newcommand{\symdiff}{\Delta}
     \newcommand{\varsymdiff}{\bigtriangleup}
     % set of bits
     \newcommand{\bits}{\{0,1\}}
     \newcommand{\sbits}{\{\pm1\}}
     % no stupid bullets for itemize environmentx
     % \renewcommand{\labelitemi}{--}
     % control white space of list and display environments
     \newcommand{\listoptions}{\labelsep0mm\topsep-0mm\itemindent-6mm\itemsep0mm}
     \newcommand{\displayoptions}[1]{\abovedisplayshortskip#1mm\belowdisplayshortskip#1mm\abovedisplayskip#1mm\belowdisplayskip#1mm}
     % short for emptyset
     %\newcommand{\eset}{\emptyset}
     % moved to mathabbreviations
     % short for epsilon
     %\newcommand{\e}{\epsilon}
     % moved to mathabbreviations
     % super index with parentheses
     \newcommand{\super}[2]{#1^{\paren{#2}}}
     \newcommand{\varsuper}[2]{#1^{\scriptscriptstyle\paren{#2}}}
     % tensor power notation
     \newcommand{\tensorpower}[2]{#1^{\tensor #2}}
     % multiplicative inverse
     \newcommand{\inv}[1]{{#1^{-1}}}
     % dual element
     \newcommand{\dual}[1]{{#1^*}}
     % subset
     %\newcommand{\sse}{\subseteq}
     % moved to mathabbreviations
     % vertical space in math formula
     \newcommand{\vbig}{\vphantom{\bigoplus}}
     % setminus
     \newcommand{\sm}{\setminus}
     % define something by an equation (display)
     \newcommand{\defeq}{\stackrel{\mathrm{def}}=}
     % define something by an equation (inline)
     \newcommand{\seteq}{\mathrel{\mathop:}=}
     % declare function f by $f \from X \to Y$
     \newcommand{\from}{\colon}
     % big middle separator (for conditioning probability spaces)
     \newcommand{\bigmid}{~\big|~}
     \newcommand{\Bigmid}{~\Big|~}
     \newcommand{\Mid}{\;\middle\vert\;}
     % better vector definition and some variations
     %\renewcommand{\vec}[1]{{\bm{#1}}}
     \newcommand{\bvec}[1]{\bar{\vec{#1}}}
     \newcommand{\pvec}[1]{\vec{#1}'}
     \newcommand{\tvec}[1]{{\tilde{\vec{#1}}}}
     % punctuation at the end of a displayed formula
     \newcommand{\mper}{\,.}
     \newcommand{\mcom}{\,,}
     % inner product for matrices
     \newcommand\bdot\bullet
     % transpose
     \newcommand{\trsp}[1]{{#1}^\dagger}
     % indicator function / vector
     \DeclareMathOperator{\Ind}{\mathbf 1}
     % place a qed symbol inside display formula
     %\qedhere
     % {{{ mathoperators }}}
     \DeclareMathOperator{\Inf}{Inf}
     \DeclareMathOperator{\Tr}{Tr}
     %\newcommand{\Tr}{\mathrm{Tr}}
     \DeclareMathOperator{\SDP}{SDP}
     \DeclareMathOperator{\sdp}{sdp}
     \DeclareMathOperator{\val}{val}
     \DeclareMathOperator{\LP}{LP}
     \DeclareMathOperator{\OPT}{OPT}
     \DeclareMathOperator{\opt}{opt}
     \DeclareMathOperator{\vol}{vol}
     \DeclareMathOperator{\poly}{poly}
     \DeclareMathOperator{\qpoly}{qpoly}
     \DeclareMathOperator{\qpolylog}{qpolylog}
     \DeclareMathOperator{\qqpoly}{qqpoly}
     \DeclareMathOperator{\argmax}{argmax}
     \DeclareMathOperator{\polylog}{polylog}
     \DeclareMathOperator{\supp}{supp}
     \DeclareMathOperator{\dist}{dist}
     \DeclareMathOperator{\sign}{sign}
     \DeclareMathOperator{\conv}{conv}
     \DeclareMathOperator{\Conv}{Conv}
     \DeclareMathOperator{\rank}{rank}
     % operators with limits
     \DeclareMathOperator*{\median}{median}
     \DeclareMathOperator*{\Median}{Median}
     % smaller summation/product symbols
     \DeclareMathOperator*{\varsum}{{\textstyle \sum}}
     \DeclareMathOperator{\tsum}{{\textstyle \sum}}
     \let\varprod\undefined
     \DeclareMathOperator*{\varprod}{{\textstyle \prod}}
     \DeclareMathOperator{\tprod}{{\textstyle \prod}}
     % {{{ textabbreviations }}}
     % some abbreviations
     \newcommand{\ie}{i.e.,\xspace}
     \newcommand{\eg}{e.g.,\xspace}
     \newcommand{\Eg}{E.g.,\xspace}
     \newcommand{\phd}{Ph.\,D.\xspace}
     \newcommand{\msc}{M.\,S.\xspace}
     \newcommand{\bsc}{B.\,S.\xspace}
     \newcommand{\etal}{et al.\xspace}
     \newcommand{\iid}{i.i.d.\xspace}
     % {{{ foreignwords }}}
     \newcommand\naive{na\"{\i}ve\xspace}
     \newcommand\Naive{Na\"{\i}ve\xspace}
     \newcommand\naively{na\"{\i}vely\xspace}
     \newcommand\Naively{Na\"{\i}vely\xspace}
     % {{{ names }}}
     % Hungarian/Polish/East European names
     \newcommand{\Erdos}{Erd\H{o}s\xspace}
     \newcommand{\Renyi}{R\'enyi\xspace}
     \newcommand{\Lovasz}{Lov\'asz\xspace}
     \newcommand{\Juhasz}{Juh\'asz\xspace}
     \newcommand{\Bollobas}{Bollob\'as\xspace}
     \newcommand{\Furedi}{F\"uredi\xspace}
     \newcommand{\Komlos}{Koml\'os\xspace}
     \newcommand{\Luczak}{\L uczak\xspace}
     \newcommand{\Kucera}{Ku\v{c}era\xspace}
     \newcommand{\Szemeredi}{Szemer\'edi\xspace}
     \newcommand{\Hastad}{H{\aa}stad\xspace}
     \newcommand{\Hoelder}{H\"{o}lder\xspace}
     \newcommand{\Holder}{\Hoelder}
     \newcommand{\Brandao}{Brand\~ao\xspace}
     % {{{ numbersets }}}
     % number sets
     \newcommand{\Z}{\mathbb Z}
     \newcommand{\N}{\mathbb N}
     \newcommand{\R}{\mathbb R}
     \newcommand{\C}{\mathbb C}
     \newcommand{\Rnn}{\R_+}
     \newcommand{\varR}{\Re}
     \newcommand{\varRnn}{\varR_+}
     \newcommand{\varvarRnn}{\R_{\ge 0}}
     % {{{ problems }}}
     % macros to denote computational problems
     % use texorpdfstring to avoid problems with hyperref (can use problem
     % macros also in headings
     \newcommand{\problemmacro}[1]{\texorpdfstring{\textup{\textsc{#1}}}{#1}\xspace}
     \newcommand{\pnum}[1]{{\footnotesize #1}}
     % list of problems
     \newcommand{\uniquegames}{\problemmacro{unique games}}
     \newcommand{\maxcut}{\problemmacro{max cut}}
     \newcommand{\multicut}{\problemmacro{multi cut}}
     \newcommand{\vertexcover}{\problemmacro{vertex cover}}
     \newcommand{\balancedseparator}{\problemmacro{balanced separator}}
     \newcommand{\maxtwosat}{\problemmacro{max \pnum{3}-sat}}
     \newcommand{\maxthreesat}{\problemmacro{max \pnum{3}-sat}}
     \newcommand{\maxthreelin}{\problemmacro{max \pnum{3}-lin}}
     \newcommand{\threesat}{\problemmacro{\pnum{3}-sat}}
     \newcommand{\labelcover}{\problemmacro{label cover}}
     \newcommand{\setcover}{\problemmacro{set cover}}
     \newcommand{\maxksat}{\problemmacro{max $k$-sat}}
     \newcommand{\mas}{\problemmacro{maximum acyclic subgraph}}
     \newcommand{\kwaycut}{\problemmacro{$k$-way cut}}
     \newcommand{\sparsestcut}{\problemmacro{sparsest cut}}
     \newcommand{\betweenness}{\problemmacro{betweenness}}
     \newcommand{\uniformsparsestcut}{\problemmacro{uniform sparsest cut}}
     \newcommand{\grothendieckproblem}{\problemmacro{Grothendieck problem}}
     \newcommand{\maxfoursat}{\problemmacro{max \pnum{4}-sat}}
     \newcommand{\maxkcsp}{\problemmacro{max $k$-csp}}
     \newcommand{\maxdicut}{\problemmacro{max dicut}}
     \newcommand{\maxcutgain}{\problemmacro{max cut gain}}
     \newcommand{\smallsetexpansion}{\problemmacro{small-set expansion}}
     \newcommand{\minbisection}{\problemmacro{min bisection}}
     \newcommand{\minimumlineararrangement}{\problemmacro{minimum linear arrangement}}
     \newcommand{\maxtwolin}{\problemmacro{max \pnum{2}-lin}}
     \newcommand{\gammamaxlin}{\problemmacro{$\Gamma$-max \pnum{2}-lin}}
     \newcommand{\basicsdp}{\problemmacro{basic sdp}}
     \newcommand{\dgames}{\problemmacro{$d$-to-1 games}}
     \newcommand{\maxclique}{\problemmacro{max clique}}
     \newcommand{\densestksubgraph}{\problemmacro{densest $k$-subgraph}}
     % {{{ alphabet }}}
     \newcommand{\cA}{\mathcal A}
     \newcommand{\cB}{\mathcal B}
     \newcommand{\cC}{\mathcal C}
     \newcommand{\cD}{\mathcal D}
     \newcommand{\cE}{\mathcal E}
     \newcommand{\cF}{\mathcal F}
     \newcommand{\cG}{\mathcal G}
     \newcommand{\cH}{\mathcal H}
     \newcommand{\cI}{\mathcal I}
     \newcommand{\cJ}{\mathcal J}
     \newcommand{\cK}{\mathcal K}
     \newcommand{\cL}{\mathcal L}
     \newcommand{\cM}{\mathcal M}
     \newcommand{\cN}{\mathcal N}
     \newcommand{\cO}{\mathcal O}
     \newcommand{\cP}{\mathcal P}
     \newcommand{\cQ}{\mathcal Q}
     \newcommand{\cR}{\mathcal R}
     \newcommand{\cS}{\mathcal S}
     \newcommand{\cT}{\mathcal T}
     \newcommand{\cU}{\mathcal U}
     \newcommand{\cV}{\mathcal V}
     \newcommand{\cW}{\mathcal W}
     \newcommand{\cX}{\mathcal X}
     \newcommand{\cY}{\mathcal Y}
     \newcommand{\cZ}{\mathcal Z}
     \newcommand{\scrA}{\mathscr A}
     \newcommand{\scrB}{\mathscr B}
     \newcommand{\scrC}{\mathscr C}
     \newcommand{\scrD}{\mathscr D}
     \newcommand{\scrE}{\mathscr E}
     \newcommand{\scrF}{\mathscr F}
     \newcommand{\scrG}{\mathscr G}
     \newcommand{\scrH}{\mathscr H}
     \newcommand{\scrI}{\mathscr I}
     \newcommand{\scrJ}{\mathscr J}
     \newcommand{\scrK}{\mathscr K}
     \newcommand{\scrL}{\mathscr L}
     \newcommand{\scrM}{\mathscr M}
     \newcommand{\scrN}{\mathscr N}
     \newcommand{\scrO}{\mathscr O}
     \newcommand{\scrP}{\mathscr P}
     \newcommand{\scrQ}{\mathscr Q}
     \newcommand{\scrR}{\mathscr R}
     \newcommand{\scrS}{\mathscr S}
     \newcommand{\scrT}{\mathscr T}
     \newcommand{\scrU}{\mathscr U}
     \newcommand{\scrV}{\mathscr V}
     \newcommand{\scrW}{\mathscr W}
     \newcommand{\scrX}{\mathscr X}
     \newcommand{\scrY}{\mathscr Y}
     \newcommand{\scrZ}{\mathscr Z}
     \newcommand{\bbB}{\mathbb B}
     \newcommand{\bbS}{\mathbb S}
     \newcommand{\bbR}{\mathbb R}
     \newcommand{\bbZ}{\mathbb Z}
     \newcommand{\bbI}{\mathbb I}
     \newcommand{\bbQ}{\mathbb Q}
     \newcommand{\bbP}{\mathbb P}
     \newcommand{\bbE}{\mathbb E}
     \newcommand{\bbN}{\mathbb N}
     \newcommand{\sfE}{\mathsf E}
     \newcommand{\calD}{\mathcal{D}}
     % {{{ leqslant }}}
     % slanted lower/greater equal signs
     \renewcommand{\leq}{\leqslant}
     \renewcommand{\le}{\leqslant}
     \renewcommand{\geq}{\geqslant}
     \renewcommand{\ge}{\geqslant}
     % {{{ varepsilon }}}
     \let\epsilon=\varepsilon
     % {{{ numberequationwithinsection }}}
     \numberwithin{equation}{section}
     % {{{ restate }}}
     % set of macros to deal with restating theorem environments (or anything
     % else with a label)
     % adapted from Boaz Barak
     \newcommand\MYcurrentlabel{xxx}
     % \MYstore{A}{B} assigns variable A value B
     \newcommand{\MYstore}[2]{%
     	\global\expandafter \def \csname MYMEMORY #1 \endcsname{#2}%
     }
     % \MYload{A} outputs value stored for variable A
     \newcommand{\MYload}[1]{%
     	\csname MYMEMORY #1 \endcsname%
     }
     % new label command, stores current label in \MYcurrentlabel
     \newcommand{\MYnewlabel}[1]{%
     	\renewcommand\MYcurrentlabel{#1}%
     	\MYoldlabel{#1}%
     }
     % new label command that doesn't do anything
     \newcommand{\MYdummylabel}[1]{}
     \newcommand{\torestate}[1]{%
     	% overwrite label command
     	\let\MYoldlabel\label%
     	\let\label\MYnewlabel%
     	#1%
     	\MYstore{\MYcurrentlabel}{#1}%
     	% restore old label command
     	\let\label\MYoldlabel%
     }
     \newcommand{\restatetheorem}[1]{%
     	% overwrite label command with dummy
     	\let\MYoldlabel\label
     	\let\label\MYdummylabel
     	\begin{theorem*}[Restatement of \cref{#1}]
     		\MYload{#1}
     	\end{theorem*}
     	\let\label\MYoldlabel
     }
     \newcommand{\restatelemma}[1]{%
     	% overwrite label command with dummy
     	\let\MYoldlabel\label
     	\let\label\MYdummylabel
     	\begin{lemma*}[Restatement of \cref{#1}]
     		\MYload{#1}
     	\end{lemma*}
     	\let\label\MYoldlabel
     }
     \newcommand{\restateprop}[1]{%
     	% overwrite label command with dummy
     	\let\MYoldlabel\label
     	\let\label\MYdummylabel
     	\begin{proposition*}[Restatement of \cref{#1}]
     		\MYload{#1}
     	\end{proposition*}
     	\let\label\MYoldlabel
     }
     \newcommand{\restatefact}[1]{%
     	% overwrite label command with dummy
     	\let\MYoldlabel\label
     	\let\label\MYdummylabel
     	\begin{fact*}[Restatement of \prettyref{#1}]
     		\MYload{#1}
     	\end{fact*}
     	\let\label\MYoldlabel
     }
     \newcommand{\restate}[1]{%
     	% overwrite label command with dummy
     	\let\MYoldlabel\label
     	\let\label\MYdummylabel
     	\MYload{#1}
     	\let\label\MYoldlabel
     }
     % {{{ mathabbreviations }}}
     \newcommand{\la}{\leftarrow}
     \newcommand{\sse}{\subseteq}
     \newcommand{\ra}{\rightarrow}
     \newcommand{\e}{\epsilon}
     \newcommand{\eps}{\epsilon}
     \newcommand{\eset}{\emptyset}
     % {{{ allowdisplaybreaks }}}
     % allows page breaks in large display math formulas
     \allowdisplaybreaks
     % {{{ sloppy }}}
     % avoid math spilling on margin
     \sloppy
     \newcommand*{\Id}{\mathrm{Id}}
     \newcommand*{\sphere}{\mathbb{S}^{n-1}}
     \newcommand*{\tr}{\mathrm{tr}}
     \newcommand*{\zo}{\set{0,1}}
     \newcommand*{\bias}{\mathrm{bias}}
     \newcommand*{\Sym}{\mathrm{sym}}
     \newcommand*{\Perm}[2]{#1^{\underline{#2}}}
     \newcommand*{\on}{\{\pm 1\}}
     \newcommand*{\sos}{\mathrm{sos}}
     \newcommand*{\U}{\mathcal{U}}
     \newcommand*{\Normop}[1]{\Norm{#1}}
     \newcommand*{\Lowner}{L\"owner\xspace}
     \newcommand*{\normtv}[1]{\Norm{#1}_{\mathrm{TV}}}
     \newcommand*{\normf}[1]{\Norm{#1}_{\mathrm{F}}}
     \DeclareMathOperator{\diag}{diag}
     \DeclareMathOperator{\pE}{\tilde{\mathbb{E}}}
     \DeclareMathOperator{\Span}{Span}
     \DeclareMathOperator{\sspan}{Span}
     % \newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
     \newcommand*{\tranpose}[1]{{#1}{}^{\mkern-1.5mu\mathsf{T}}}
     \newcommand*{\dyad}[1]{#1#1{}^{\mkern-1.5mu\mathsf{T}}}
     \newcommand{\1}{\bm{1}}
     
     \newcommand{\wh}{\widehat}
     \newcommand{\tmu}{\tilde{\mu}}
     \newcommand{\wt}{\mathsf{wt}}
     \newcommand{\cond}{\mathsf{cond}}
     \renewcommand{\S}{\mathbb{S}}
     \newcommand{\Sol}{\mathrm{Sol}}
     \newcommand{\In}{\mathcal{I}}
     \newcommand{\Ou}{\mathcal{O}}
     \newcommand{\Lin}{\mathrm{Lin}}
     % TITLE
     

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fontst
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{List-decodeable Linear Regression}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
We give the first polynomial-time algorithm for robust regression in the list-decodable setting where an adversary can corrupt a greater than $1/2$ fraction of examples. %outliers investigated in several recent works~\cite{DBLP:conf/stoc/CharikarSV17,KothariSteinhardt17,DBLP:conf/stoc/DiakonikolasKS18}.
For any $\alpha < 1$, our algorithm takes as input a sample $\{ (x_i,y_i)\}_{i \leq n}$ of $n$ linear equations where $\alpha n$ of the equations satisfy $y_i = \langle x_i,\ell^*\rangle +\zeta$ for some small noise $\zeta$ and $(1-\alpha)n$ of the equations are {\em arbitrarily} chosen. It outputs a list $L$ of size $O(1/\alpha)$ - a fixed constant - that contains an $\ell$ that is close to $\ell^*$.

Our algorithm succeeds whenever the inliers are chosen from a \emph{certifiably} anti-concentrated distribution $D$. To  complement our result, we prove that the anti-concentration assumption on the inliers is information-theoretically necessary. As a corollary of our algorithmic result, we obtain a $(d/\alpha)^{O(1/\alpha^8)}$ time algorithm to find a $O(1/\alpha)$ size list when the inlier distribution is standard Gaussian. For discrete product distributions that are anti-concentrated only in \emph{regular} directions, we give an algorithm that achieves similar guarantee under the promise that $\ell^*$ has all coordinates of same magnitude.
%#distribution (and more generally, any spherically symmetric distribution with sub-exponential tails and linear-transformations thereof).


To solve the problem we introduce a new framework for list-decodable learning that strengthens the ``identifiability to algorithms'' paradigm based on the sum-of-squares method.

In an independent work, Raghavendra and Yau~\cite{RY19} have obtained a similar result for list-decodable regression also using the sum-of-squares method.
\end{abstract}

\section{Introduction}

In this work, we design algorithms for the problem of linear regression that are robust to training sets with an overwhelming ($\gg 1/2$) fraction of adversarially chosen outliers. 

Outlier-robust learning algorithms have been extensively studied (under the name \emph{robust statistics}) in mathematical statistics~\cite{MR0426989,maronna2006robust,huber2011robust,hampel2011robust}. However, the algorithms resulting from this line of work usually run in time exponential in the dimension of the data~\cite{bernholt2006robust}. An influential line of recent work \cite{journals/jmlr/KlivansLS09,journals/corr/AwasthiBL13, DBLP:journals/corr/DiakonikolasKKL16,DBLP:conf/focs/LaiRV16,DBLP:conf/stoc/CharikarSV17,KothariSteinhardt17,2017KS,HopkinsLi17,DBLP:journals/corr/DiakonikolasKK017a,DBLP:journals/corr/DiakonikolasKS17,DBLP:conf/colt/KlivansKM18} has focused on designing \emph{efficient} algorithms for outlier-robust learning. 

Our work extends this line of research. Our algorithms work in the \emph{list-decodable learning} framework. In this model, the majority of the training data (a $1 -\alpha$ fraction) can be adversarially corrupted leaving only an $\alpha \ll 1/2$ fraction of \emph{inliers}. Since uniquely recovering the underlying parameters is information-theoretically \emph{impossible} in such a setting, the goal is to output a list (with an absolute constant size) of parameters, one of which matches the ground truth. This model was introduced in~\cite{DBLP:conf/stoc/BalcanBV08} to give a discriminative framework for clustering. More recently, beginning with~\cite{DBLP:conf/stoc/CharikarSV17}, various works~\cite{DBLP:conf/stoc/DiakonikolasKS18,KothariSteinhardt17} have considered this as a model of \emph{untrusted} data. 

There has been a phenomenal progress in developing techniques for outlier-robust learning with a \emph{small} $(\ll 1/2)$-fraction of outliers (e.g. outlier \emph{filters}~\cite{DBLP:conf/focs/DiakonikolasKK016,DBLP:journals/corr/DiakonikolasKK017a}, separation oracles for inliers~\cite{DBLP:conf/focs/DiakonikolasKK016} or the \emph{sum-of-squares} method~\cite{2017KS,HopkinsLi17,KothariSteinhardt17,DBLP:conf/colt/KlivansKM18}). In contrast, progress on algorithms that tolerate the significantly harsher conditions in the list-decodable setting has been slower. The only prior works~\cite{DBLP:conf/stoc/CharikarSV17,DBLP:conf/stoc/DiakonikolasKS18,KothariSteinhardt17} in this direction designed list-decodable algorithms for mean estimation via somewhat \emph{ad hoc}, problem-specific methods. 

In this paper, we develop a principled technique to give the first efficient list-decodable learning algorithm for the fundamental problem of \emph{linear regression}. Our algorithm takes a corrupted set of linear equations with an $\alpha \ll 1/2$ fraction of inliers and outputs a $O(1/\alpha)$-size list of linear functions, one of which is guaranteed to be close to the ground truth (i.e., the linear function that correctly labels the inliers). Our key conceptual observation shows that list-decodable regression information-theoretically requires the inlier-distribution to be \emph{anti-concentrated}. Our algorithm succeeds whenever the distribution satisfies a stronger \emph{certifiable anti-concentration} condition. This class includes the standard gaussian distribution and more generally, any spherically symmetric distribution with strictly sub-exponential tails.

Prior to our work\footnote{There's a long line of work on robust regression algorithms (see for e.g. \cite{DBLP:conf/nips/Bhatia0KK17,conf/soda/KarmalkarP19}) that can tolerate corruptions only in the \emph{labels}. We are interested in algorithms robust against corruptions in both examples and labels.}, the state-of-the-art outlier-robust algorithms for linear regression~\cite{DBLP:conf/colt/KlivansKM18,conf/soda/DiakonikolasKS19,journals/corr/abs-1803-02815,journals/corr/abs-1802-06485} could handle only a small $(<0.1)$-fraction of outliers even under strong assumptions on the underlying distributions. 

List-decodable regression generalizes the well-studied~\cite{MR1028403,doi:10.1162/neco.1994.6.2.181,MR2757044,2013arXiv1310.3745Y,DBLP:journals/corr/BalakrishnanWY14,DBLP:conf/colt/ChenYC14,DBLP:conf/nips/Zhong0D16,DBLP:conf/aistats/SedghiJA16,DBLP:conf/colt/LiL18} and {\em easier} problem of \emph{mixed linear regression}: given $k$ ``clusters'' of examples that are labeled by one out of $k$ distinct unknown linear functions, find the unknown set of linear functions. All known techniques for the problem rely on faithfully estimating \emph{moment tensors} from samples and thus, cannot tolerate the overwhelming fraction of outliers in the list-decodable setting. On the other hand, since we can take any cluster as inliers and treat rest as outliers, our algorithm immediately yields new efficient algorithms for mixed linear regression. Unlike all prior works, our algorithms work without any pairwise separation or bounded condition-number assumptions on the $k$ linear functions. 








% The MLR model is a popular mixture model and has many applications due to its effectiveness in captur- ing non-linearity and its model simplicity [De Veaux, 1989, Jordan and Jacobs, 1994, Faria and Soromenho, 2010, Zhong et al., 2016]. It has also been a recent theoretical topic for analyzing benchmark algorithms for nonconvex optimization (e.g., [Chaganty and Liang, 2013, Klusowski et al., 2017]) or designing new algo- rithms (e.g., [Chen et al., 2014]). However, most of the existing works either restrict to very special settings (e.g., x of different components all from the standard Gaussian, or only k = 2 components) [Chen et al., 2014, Yi et al., 2014, Zhong et al., 2016, Balakrishnan et al., 2017, Klusowski et al., 2017], or have high sample or computational complexity far from optimal [Chaganty and Liang, 2013, Sedghi et al., 2016].



\paragraph{List-Decodable Learning via the Sum-of-Squares Method} Our algorithm relies on a strengthening of the robust-estimation framework based on the sum-of-squares (SoS) method. This paradigm has been recently used for clustering mixture models~\cite{HopkinsLi17,KothariSteinhardt17} and obtaining algorithms for moment estimation~\cite{2017KS} and linear regression~\cite{DBLP:conf/colt/KlivansKM18}
relies on a strengthening of robust-estimation framework based on the sum-of-squares (SoS) method. This paradigm has been recently used for clustering mixture models~\cite{HopkinsLi17,KothariSteinhardt17} and obtaining algorithms for moment estimation~\cite{2017KS} and linear regression~\cite{DBLP:conf/colt/KlivansKM18} that are resilient to a small $(\ll 1/2)$ fraction of outliers under the mildest known assumptions on the underlying distributions. Ths method reduces outlier-robust algorithm design to finding ``simple'' proofs of  unique \emph{identifiability} of the unknown parameter of the original distribution from a corrupted sample. However, this principled method works only in the setting with a small ($\ll 1/2$) fraction of outliers. As a consequence, the work of~\cite{KothariSteinhardt17} for mean estimation in the list-decodable setting relied on ``supplementing'' the SoS method with a somewhat \emph{ad hoc}, problem-dependent technique. 

As an important conceptual contribution, our work yields a framework for list-decodable learning that recovers some of the simplicity of the general blueprint. To do this, we give a general method for \emph{rounding pseudo-distributions} in the setting with $\gg 1/2$ fraction outliers. A key step in our rounding builds on the work of~\cite{KS19} who developed such a method to give a simpler proof of the list-decodable mean estimation result of~\cite{KothariSteinhardt17}. In Section~\ref{sec:overview}, we explain our ideas in detail. 

The results in all the works above hold whenever the underlying distribution satisfies a certain \emph{certified concentration} condition formulated within the SoS system via higher moment bounds. An important contribution of this work is formalizing an \emph{anti-concentration} condition within the SoS system. Unlike the bounded moment condition, there is no canonical phrasing  within SoS for such statements. We choose a form that allows proving ``certified anti-concentration'' for a distribution by showing the existence of a certain approximating polynomial. This allows showing certified anti-concentration of natural distributions via a completely modular approach that relies on a beautiful line of works that construct ``weighted '' polynomial approximators~\cite{2007math......1099L}. 

We believe that our framework for list-decodable estimation and our formulation of certified anti-concentration condition will likely have further applications in outlier-robust learning. 
% We focus on the setting where the noise rate is larger than $1/2$ and unique recovery of the underlying parameters is information-theoretically {\em impossible}.  We study the supervised learning problem of linear regression and assume that a learner has been given a training set where a (possibly greater than $1/2$) fraction of examples have been {\em arbitrarily} corrupted in both the labels and locations of the data points.  Given this training set as input, we give the first efficient algorithm for outputting a {\em list} of linear functions, one of which agrees with the original, uncorrupted data set (the list is {\em constant-size} where the constant depends on the noise rate).  We refer to this problem as {\em list-decodable linear regression}\footnote{Our setting is similar in spirit to list-decodable error correcting codes, but the problem statements and techniques are quite different.}.  It is easy to see that list-decodable linear regression is harder than classical {\em mixed linear regression}, an intensely studied problem in machine learning.  Our algorithm requires an assumption on the marginal distribution, namely {\em certifiable anti-concentration}, which we prove is information-theoretically necessary for list-decodability in this setting.  
\subsection{Our Results}
We first define our model for generating samples for list-decodable regression.

\begin{model}[Robust Linear Regression]
	For $0 <\alpha < 1$ and $\ell^* \in \R^d$ with $\|\ell^*\|_2 \leq 1$, let $\Lin_D(\alpha,\ell^*)$ denote the following probabilistic process to generate $n$ noisy linear equations $\cS = \{ \langle x_i, a \rangle = y_i\mid 1\leq i \leq n\}$ in variable $a \in \R^d$ with $\alpha n$ \emph{inliers} $\cI$ and $(1-\alpha)n$ \emph{outliers} $\cO$:
	\begin{enumerate}
		\item Construct $\cI$ by choosing $\alpha n$ i.i.d. samples $x_i \sim D$ and set $y_i = \langle x_i,\ell^* \rangle + \zeta$ for additive and independent noise $\zeta$,
		\item Construct $\cO$ by  choosing the remaining $(1-\alpha)n$ equations arbitrarily and potentially adversarially w.r.t the inliers $\cI$.
	\end{enumerate}
	\label{model:random-equations}
\end{model}
The bound on the norm of $\ell^*$ is without any loss of generality. 
% Our algorithm naturally extends to handle additive noise in the labels. 
% We will focus mostly on the noiseless case for clarity of exposition.
Note that $\alpha$ is measure of the ``signal'' (fraction of inliers) and can be $\ll 1/2$. 

An $\eta$-approximate algorithm for list-decodable regression takes input a sample from $\Lin_D(\alpha,\ell^*)$ and outputs a \emph{constant} (depending only on $\alpha$) size list $L$ of linear functions such that there is some $\ell \in L$ that is $\eta$-close to $\ell^*$.

One of our key conceptual contributions is to identify the strong relationship between \emph{anti-concentration inequalities} and list-decodable regression. Anti-concentration inequalities are well-studied~\cite{ErdosLittlewoodOfford,MR2965282-Tao12,MR2407948-Rudelson08} in probability theory and combinatorics. The simplest of these inequalities upper bound the probability that a high-dimensional random variable has zero projections in any direction.

\begin{definition}[Anti-Concentration]
	A $\R^d$-valued zero-mean random variable $Y$ has a $\delta$-\emph{anti-concentrated} distribution if $\Pr[ \iprod{Y,v}=0 ]< \delta$. 
\end{definition}

In Proposition~\ref{prop:identifiability}, we provide a simple but conceptually illuminating proof that anti-concentration is \emph{sufficient} for list-decodable regression. In Theorem~\ref{thm:main-lower-bound}, we prove a sharp converse and show that anti-concentration is information-theoretically \emph{necessary} for even noiseless list-decodable regression. This lower bound surprisingly holds for a natural distribution: uniform distribution on $\zo^d$ and more generally, uniform distribution on $[q]^d$ for  $q = \{0,1,2\ldots,q\}$.

\begin{theorem}[See Proposition~\ref{prop:identifiability} and Theorem~\ref{thm:main-lower-bound}]
	There is a (inefficient) list-decodable regression algorithm for $\Lin_D(\alpha,\ell^*)$ with list size $O(\frac{1}{\alpha})$ whenever $D$ is $\alpha$-anti-concentrated. 
	Further, there exists a distribution $D$ on $\R^d$ that is $\paren{\alpha+\epsilon}$-anti-concentrated for every $\epsilon >0$ but there is no algorithm for $\frac{\alpha}{2}$-approximate list-decodable regression for $\Lin_D(\alpha,\ell^*)$  that returns a list of size $<d$. 
\end{theorem}
For our efficient algorithms, we need a \emph{certified} version of the anti-concentration condition. 
To handle additive noise of variance $\zeta^2$, we need a control of $\Pr[ |\iprod{x,v}| \leq \zeta]$. 
Thus, we extend our notion of anti-concentration and then define a \emph{certified} analog of it:
\begin{definition}[Certifiable Anti-Concentration] \label{def:certified-anti-concentration}
	A random variable $Y$ has a $k$-\emph{certifiably} $(C,\delta)$-anti-concentrated distribution if there is a univariate polynomial $p$ satisfying $p(0) = 1$ such that there is a degree $k$ sum-of-squares proof of the following two inequalities: 
	\begin{enumerate} 
		\item $\forall v$, $\langle Y,v\rangle^2 \leq \delta^2 \E \langle Y,v\rangle^2$ implies $(p(\langle Y,v\rangle) -1)^2\leq \delta^2$.
		\item $\forall v$, $\|v\|_2^2 \leq 1$ implies  $\E p^2(\left \langle Y,v\rangle \right) \leq C\delta$.
	\end{enumerate}
\end{definition} 
Intuitively, certified anti-concentration asks for a \emph{certificate} of the anti-concentration property of $Y$ in the ``sum-of-squares'' proof system (see Section~\ref{sec:preliminaries} for precise definitions). SoS is a proof system that reasons about  polynomial inequalities. Since the ``core indicator'' $\1(|\iprod{x,v}| \leq \delta)$ is not a polynomial, we phrase the condition in terms of an approximating polynomial $p$.
\Pnote{seems like it would be nice to have some explanation for the properties of the polynomial asked for in this definition...}
We are now ready to state our main result.
\begin{restatable}[List-Decodable Regression]{theorem}{main} \label{thm:main}
	For every $\alpha, \eta > 0$ and a $k$-certifiably $(C,\alpha^2 \eta^2/10C)$-anti-concentrated distribution $D$ on $\R^d$, there exists an algorithm that takes input a sample generated according to $\Lin_D(\alpha,\ell^*)$ and outputs a list $L$ of size $O(1/\alpha)$ such that there is an $\ell \in L$ satisfying $\| \ell - \ell^*\|_2 < \eta$ with probability at least $0.99$ over the draw of the sample. The algorithm needs a sample of size $n = (kd)^{O(k)}$ and runs in time $n^{O(k)} = (kd)^{O(k^2)}$.
\end{restatable} 
\begin{remark}[Tolerating Additive Noise]
	For additive noise of variance $\zeta^2$ in the inlier labels, our algorithm, in the same running time and sample complexity, outputs a list of size $O(1/\alpha)$ that contains an $\ell$ satisfying $\|\ell-\ell^*\|_2 \leq \frac{\zeta}{\alpha} + \eta$. Since we normalize $\ell^*$ to have unit norm, this guarantee is meaningful only when $\zeta \ll \alpha$. %It is not clear if better noise-tolerance is possible for list-decodable regression.
\end{remark}

\begin{remark}[Exponential Dependence on $1/\alpha$]
	List-decodable regression algorithms immediately yield algorithms for mixed linear regression (MLR) without any assumptions on the components. The state-of-the-art algorithm for MLR with gaussian components~\cite{DBLP:conf/colt/LiL18} has an exponential dependence on $k=1/\alpha$ in the running time in the absence of strong pairwise separation or small condition number of the components. Liang and Liu~\cite{DBLP:conf/colt/LiL18} (see Page 10) use the relationship to learning mixtures of $k$ gaussians (with an $\exp(k)$ lower bound~\cite{DBLP:conf/focs/MoitraV10}) to note that algorithms with polynomial dependence on $1/\alpha$ for MLR and thus, also for list-decodable regression might not exist. 
\end{remark}


\paragraph{Certifiably anti-concentrated distributions} In Section~\ref{sec:certified-anti-concentration}, we show certifiable anti-concentration of some well-studied families of distributions. This includes the standard gaussian distribution and more generally any anti-concentrated spherically symmetric distribution with strictly sub-exponential tails. We also show that simple operations such as scaling, applying well-conditioned linear transformations and sampling preserve certifiable anti-concentration. This yields:
\begin{corollary}[List-Decodable Regression for Gaussian Inliers]
	For every $\alpha, \eta > 0$ there's an algorithm for list-decodable regression for the model $\Lin_D(\alpha,\ell^*)$ with $D = \cN(0,\Sigma)$ with $\lambda_{\max}(\Sigma)/\lambda_{min}(\Sigma) = O(1)$ that needs $n = (d/\alpha \eta)^{O\left(\frac{1}{\alpha^4 \eta^4}\right)}$  samples and runs in time $n^{O\left(\frac{1}{\alpha^4 \eta^4}\right)} = (d/\alpha \eta)^{O\left(\frac{1}{\alpha^8 \eta^8}\right)}$.
\end{corollary} 

We note that certifiably anti-concentrated distributions are more restrictive compared to the families of distributions for which the most general robust estimation algorithms work~\cite{2017KS,KothariSteinhardt17,DBLP:conf/colt/KlivansKM18}. To a certain extent, this is inherent. The families of distributions considered in these prior works do not satisfy anti-concentration in general.  And as we discuss in more detail in Section~\ref{sec:overview}, anti-concentration is information-theoretically \emph{necessary} (see Theorem~\ref{thm:main-lower-bound}) for list-decodable regression. This surprisingly rules out families of distributions that might appear natural and ``easy'', for example, the uniform distribution on $\zo^n$. In fact, our lower bound shows the impossibility of even the ``easier'' problem of mixed linear regression on this distribution.

We rescue this to an extent for the special case when $\ell^*$ in the model $\Lin(\alpha,\ell^*)$ is a "Boolean vector", i.e., has all coordinates of equal magnitude. Intuitively, this helps because  while the the uniform distribution on $\zo^n$ (and more generally, any discrete product distribution) is badly anti-concentrated in sparse directions, they are well anti-concentrated~\cite{ErdosLittlewoodOfford} in the directions that are far from any sparse vectors. 

As before, for obtaining efficient algorithms, we need to work with a \emph{certified} version (see Definition~\ref{def:certified-anti-concentration-Boolean}) of such a restricted anti-concentration condition. As a specific Corollary (see Theorem~\ref{thm:Booleanmain} for a more general statement), this allows us to show:
\begin{theorem}[List-Decodable Regression for Hypercube Inliers] \label{thm:boolcube}
	For every $\alpha, \eta > 0$ there's an $\eta$-approximate algorithm for list-decodable regression for the model $\Lin_D(\alpha,\ell^*)$ with $D$ is uniform on $\zo^d$ that needs $n = (d/\alpha \eta)^{O(\frac{1}{\alpha^4 \eta^4})}$  samples and runs in time $n^{O(\frac{1}{\alpha^4 \eta^4})} = (d/\alpha \eta)^{O(\frac{1}{\alpha^8 \eta^8})}$.
\end{theorem} 

In Section~\ref{sec:hypercube}, we obtain similar results for general product distributions. It is an important open problem to prove certified anti-concentration for a broader family of distributions. % In particular, this yields a $d^{O(\frac{1}{\alpha^2 \eta^2})}$-sample and $d^{O(\frac{1}{\alpha^4 \eta^4})}$-time algorithm for list-decodable regression when $D$ is the uniform distribution on the Boolean hypercube/solid hypercube. 


\subsection{Concurrent Work}
In an independent and concurrent work, Raghavendra and Yau have given similar results for list-decodable linear regression and also use the sum-of-squares paradigm \cite{RY19}.

\section{Preliminaries}
\label{sec:preliminaries}

In this section, we define pseudo-distributions and sum-of-squares proofs.
See the lecture notes \cite{BarakS16} for more details and the appendix in \cite{DBLP:conf/focs/MaSS16} for proofs of the propositions appearing here.

Let $x = (x_1, x_2, \ldots, x_n)$ be a tuple of $n$ indeterminates and let $\R[x]$ be the set of polynomials with real coefficients and indeterminates $x_1,\ldots,x_n$.
We say that a polynomial $p\in \R[x]$ is a \emph{sum-of-squares (sos)} if there are polynomials $q_1,\ldots,q_r$ such that $p=q_1^2 + \cdots + q_r^2$.

\subsection{Pseudo-distributions}

Pseudo-distributions are generalizations of probability distributions.
We can represent a discrete (i.e., finitely supported) probability distribution over $\R^n$ by its probability mass function $D\from \R^n \to \R$ such that $D \geq 0$ and $\sum_{x \in \mathrm{supp}(D)} D(x) = 1$.
Similarly, we can describe a pseudo-distribution by its mass function.
Here, we relax the constraint $D\ge 0$ and only require that $D$ passes certain low-degree non-negativity tests.

Concretely, a \emph{level-$\ell$ pseudo-distribution} is a finitely-supported function $D:\R^n \rightarrow \R$ such that $\sum_{x} D(x) = 1$ and $\sum_{x} D(x) f(x)^2 \geq 0$ for every polynomial $f$ of degree at most $\ell/2$.
(Here, the summations are over the support of $D$.)
A straightforward polynomial-interpolation argument shows that every level-$\infty$-pseudo distribution satisfies $D\ge 0$ and is thus an actual probability distribution.
We define the \emph{pseudo-expectation} of a function $f$ on $\R^d$ with respect to a pseudo-distribution $D$, denoted $\pE_{D(x)} f(x)$, as
\begin{equation}
\pE_{D(x)} f(x) = \sum_{x} D(x) f(x) \,\mper
\end{equation}
The degree-$\ell$ moment tensor of a pseudo-distribution $D$ is the tensor $\E_{D(x)} (1,x_1, x_2,\ldots, x_n)^{\otimes \ell}$.
In particular, the moment tensor has an entry corresponding to the pseudo-expectation of all monomials of degree at most $\ell$ in $x$.
The set of all degree-$\ell$ moment tensors of probability distribution is a convex set.
Similarly, the set of all degree-$\ell$ moment tensors of degree $d$ pseudo-distributions is also convex.
Key to the algorithmic utility of pseudo-distributions is the fact that while there can be no efficient separation oracle for the convex set of all degree-$\ell$ moment tensors of an actual probability distribution, there's a separation oracle running in time $n^{O(\ell)}$ for the convex set of the degree-$\ell$ moment tensors of all level-$\ell$ pseudodistributions.

\begin{fact}[\cite{MR939596-Shor87,parrilo2000structured,MR1748764-Nesterov00,MR1846160-Lasserre01}]
	\label[fact]{fact:sos-separation-efficient}
	For any $n,\ell \in \N$, the following set has a $n^{O(\ell)}$-time weak separation oracle (in the sense of \cite{MR625550-Grotschel81}):
	\begin{equation}
	\Set{ \pE_{D(x)} (1,x_1, x_2, \ldots, x_n)^{\otimes d} \mid \text{ degree-d pseudo-distribution $D$ over $\R^n$}}\,\mper
	\end{equation}
\end{fact}
This fact, together with the equivalence of weak separation and optimization \cite{MR625550-Grotschel81} allows us to efficiently optimize over pseudo-distributions (approximately)---this algorithm is referred to as the sum-of-squares algorithm.

The \emph{level-$\ell$ sum-of-squares algorithm} optimizes over the space of all level-$\ell$ pseudo-distributions that satisfy a given set of polynomial constraints---we formally define this next.

\begin{definition}[Constrained pseudo-distributions]
	Let $D$ be a level-$\ell$ pseudo-distribution over $\R^n$.
	Let $\cA = \{f_1\ge 0, f_2\ge 0, \ldots, f_m\ge 0\}$ be a system of $m$ polynomial inequality constraints.
	We say that \emph{$D$ satisfies the system of constraints $\cA$ at degree $r$}, denoted $D \sdtstile{r}{} \cA$, if for every $S\subseteq[m]$ and every sum-of-squares polynomial $h$ with $\deg h + \sum_{i\in S} \max\set{\deg f_i,r}$,
	\begin{displaymath}
	\pE_{D} h \cdot \prod _{i\in S}f_i  \ge 0\,.
	\end{displaymath}
	We write $D \sdtstile{}{} \cA$ (without specifying the degree) if $D \sdtstile{0}{} \cA$ holds.
	Furthermore, we say that $D\sdtstile{r}{}\cA$ holds \emph{approximately} if the above inequalities are satisfied up to an error of $2^{-n^\ell}\cdot \norm{h}\cdot\prod_{i\in S}\norm{f_i}$, where $\norm{\cdot}$ denotes the Euclidean norm\footnote{The choice of norm is not important here because the factor $2^{-n^\ell}$ swamps the effects of choosing another norm.} of the cofficients of a polynomial in the monomial basis.
\end{definition}

We remark that if $D$ is an actual (discrete) probability distribution, then we have  $D\sdtstile{}{}\cA$ if and only if $D$ is supported on solutions to the constraints $\cA$.

We say that a system $\cA$ of polynomial constraints is \emph{explicitly bounded} if it contains a constraint of the form $\{ \|x\|^2 \leq M\}$.
The following fact is a consequence of \cref{fact:sos-separation-efficient} and \cite{MR625550-Grotschel81},

\begin{fact}[Efficient Optimization over Pseudo-distributions]
	There exists an $(n+ m)^{O(\ell)} $-time algorithm that, given any explicitly bounded and satisfiable system\footnote{Here, we assume that the bitcomplexity of the constraints in $\cA$ is $(n+m)^{O(1)}$.} $\cA$ of $m$ polynomial constraints in $n$ variables, outputs a level-$\ell$ pseudo-distribution that satisfies $\cA$ approximately. \label{fact:eff-pseudo-distribution}
\end{fact}

\subsection{Sum-of-squares proofs}

Let $f_1, f_2, \ldots, f_r$ and $g$ be multivariate polynomials in $x$.
A \emph{sum-of-squares proof} that the constraints $\{f_1 \geq 0, \ldots, f_m \geq 0\}$ imply the constraint $\{g \geq 0\}$ consists of  polynomials $(p_S)_{S \subseteq [m]}$ such that
\begin{equation}
g = \sum_{S \subseteq [m]} p_S \cdot \Pi_{i \in S} f_i
\mper
\end{equation}
We say that this proof has \emph{degree $\ell$} if for every set $S \subseteq [m]$, the polynomial $p_S \Pi_{i \in S} f_i$ has degree at most $\ell$.
If there is a degree $\ell$ SoS proof that $\{f_i \geq 0 \mid i \leq r\}$ implies $\{g \geq 0\}$, we write:
\begin{equation}
\{f_i \geq 0 \mid i \leq r\} \sststile{\ell}{}\{g \geq 0\}
\mper
\end{equation}


Sum-of-squares proofs satisfy the following inference rules.
For all polynomials $f,g\colon\R^n \to \R$ and for all functions $F\colon \R^n \to \R^m$, $G\colon \R^n \to \R^k$, $H\colon \R^{p} \to \R^n$ such that each of the coordinates of the outputs are polynomials of the inputs, we have:

\begin{align}
&\frac{\cA \sststile{\ell}{} \{f \geq 0, g \geq 0 \} } {\cA \sststile{\ell}{} \{f + g \geq 0\}}, \frac{\cA \sststile{\ell}{} \{f \geq 0\}, \cA \sststile{\ell'}{} \{g \geq 0\}} {\cA \sststile{\ell+\ell'}{} \{f \cdot g \geq 0\}} \tag{addition and multiplication}\\
&\frac{\cA \sststile{\ell}{} \cB, \cB \sststile{\ell'}{} C}{\cA \sststile{\ell \cdot \ell'}{} C}  \tag{transitivity}\\
&\frac{\{F \geq 0\} \sststile{\ell}{} \{G \geq 0\}}{\{F(H) \geq 0\} \sststile{\ell \cdot \deg(H)} {} \{G(H) \geq 0\}} \tag{substitution}\mper
\end{align}

Low-degree sum-of-squares proofs are sound and complete if we take low-level pseudo-distributions as models.

Concretely, sum-of-squares proofs allow us to deduce properties of pseudo-distributions that satisfy some constraints.

\begin{fact}[Soundness]
	\label{fact:sos-soundness}
	If $D \sdtstile{r}{} \cA$ for a level-$\ell$ pseudo-distribution $D$ and there exists a sum-of-squares proof $\cA \sststile{r'}{} \cB$, then $D \sdtstile{r\cdot r'+r'}{} \cB$.
\end{fact}

If the pseudo-distribution $D$ satisfies $\cA$ only approximately, soundness continues to hold if we require an upper bound on the bit-complexity of the sum-of-squares $\cA \sststile{r'}{} B$  (number of bits required to write down the proof).

In our applications, the bit complexity of all sum of squares proofs will be $n^{O(\ell)}$ (assuming that all numbers in the input have bit complexity $n^{O(1)}$).
This bound suffices in order to argue about pseudo-distributions that satisfy polynomial constraints approximately.

The following fact shows that every property of low-level pseudo-distributions can be derived by low-degree sum-of-squares proofs.

\begin{fact}[Completeness]
	\label{fact:sos-completeness}
	Suppose $d \geq r' \geq r$ and $\cA$ is a collection of polynomial constraints with degree at most $r$, and $\cA \vdash \{ \sum_{i = 1}^n x_i^2 \leq B\}$ for some finite $B$.
	
	Let $\{g \geq 0 \}$ be a polynomial constraint.
	If every degree-$d$ pseudo-distribution that satisfies $D \sdtstile{r}{} \cA$ also satisfies $D \sdtstile{r'}{} \{g \geq 0 \}$, then for every $\epsilon > 0$, there is a sum-of-squares proof $\cA \sststile{d}{} \{g \geq - \epsilon \}$.
\end{fact}

We will use the following Cauchy-Schwarz inequality for pseudo-distributions:

\begin{fact}[Cauchy-Schwarz for Pseudo-distributions]
	Let $f,g$ be polynomials of degree at most $d$ in indeterminate $x \in \R^d$. Then, for any degree d pseudo-distribution $\tmu$,
	$\pE_{\tmu}[fg] \leq \sqrt{\pE_{\tmu}[f^2]} \sqrt{\pE_{\tmu}[g^2]}$.
	\label{fact:pseudo-expectation-cauchy-schwarz}
\end{fact} 

The following fact is a simple corollary of the fundamental theorem of algebra:
\begin{fact}
	For any univariate degree $d$ polynomial $p(x) \geq 0$ for all $x \in \R$, 
	$\sststile{d}{x} \Set{p(x) \geq 0}$.
	\label{fact:univariate}
\end{fact}

This can be extended to univariate polynomial inequalities over intervals of $\R$. 

\begin{fact}[Fekete and Markov-Lukcs, see \cite{laurent2009sums}]
	For any univariate degree $d$ polynomial $p(x) \geq 0$ for $x \in [a, b]$,  $\Set{x\geq a, x \leq b} \sststile{d}{x} \Set{p(x) \geq 0}$.  \label{fact:univariate-interval}
\end{fact}



\phantomsection
\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{bib/mathreview,bib/dblp,bib/custom,bib/scholar,bib/main}

\end{document}